{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AjayRahulRaja/Machine_Learning_Neural_Networks/blob/main/Keras_Assignment_Dec2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VntVQjqChY1T"
      },
      "source": [
        "# CW1 - Multimodal IMDB Analysis with Keras\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this assignment you will be provided a dataset containing a selection of films together with their posters in JPEG image format and their overviews in text format from the Internet Movie Database.\n",
        "\n",
        "![Example](https://i.ibb.co/J3trT62/Screenshot-2024-09-22-214753.png)\n",
        "\n",
        "You will be analysing this dataset by implementing and training two models: a **CNN** and an **LSTM**.\n",
        "\n",
        "The CNN must classify film posters by the genre. Independently, the LSTM must classify film overviews by the genre. Finally, you will evaluate and critically comment your results in a short report. (Which of the two models was better at classifying films?)\n",
        "\n",
        "## Structure of the assignment\n",
        "\n",
        "This assignment is broken up into sections and you need to complete each section successively. The sections are the following:\n",
        "\n",
        "1. Data Processing\n",
        "\n",
        "  1.a. Image processing of the posters\n",
        "\n",
        "  1.b. Natural language processing of the overviews\n",
        "\n",
        "2. Definition of the models\n",
        "\n",
        "  2.a. CNN for the posters\n",
        "\n",
        "  2.b. LSTM for the overviews\n",
        "\n",
        "3. Training of the models\n",
        "4. Evaluation of the models\n",
        "\n",
        "In addition to this coding exercise, you must write a **2-3 pages** report analysing and critically evaluating your model's results. Marks for the report will be awarded for depth of analysis and critical thinking skills. You should consider how well your model performs and WHY it does that—give specific examples and comment on their importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eBNRzib8voMm"
      },
      "outputs": [],
      "source": [
        "# Enter your module imports here, some modules are already provided\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dropout, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGHk085OWCT_",
        "outputId": "81f6d237-7974-4765-852b-5d2c6b705586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Init1\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN-7QYYKKCOH"
      },
      "source": [
        "# 1. Data Processing\n",
        "\n",
        "Warning: running the following cell can take some time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9yWqczq8KGI6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d283c950-024e-4bae-8af0-353fbe1a8efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        192.13M 100%    1.01MB/s    0:03:01 (xfr#7916, to-chk=0/7917)\n"
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Init2\n",
        "# Do not edit, remove, or copy this cell\n",
        "\n",
        "# This code will copy the images from your google drive into the colab file\n",
        "# storage. Make sure the dataset is unzipped in your drive folder.\n",
        "! mkdir /tmp/Multimodal_IMDB_dataset\n",
        "! rsync -ah --info=progress2 /content/drive/MyDrive/Multimodal_IMDB_dataset/Images /tmp/Multimodal_IMDB_dataset/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gcvpy52m14oB"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init3\n",
        "# Do not edit, remove, or copy this cell\n",
        "\n",
        "# Load the csv with the sample IDs and genres\n",
        "csv_loc = \"/content/drive/MyDrive/Multimodal_IMDB_dataset/IMDB_overview_genres.csv\"\n",
        "dataset = pd.read_csv(csv_loc)\n",
        "\n",
        "# Split the dataset into training and testing\n",
        "data_train, data_test = model_selection.train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "# Convert the labels to arrays\n",
        "labels_train = np.array(data_train.drop(['Id', 'Genre', 'Overview'],axis=1)).astype('float32')\n",
        "labels_test = np.array(data_test.drop(['Id', 'Genre', 'Overview'],axis=1)).astype('float32')\n",
        "\n",
        "# List of the genre names\n",
        "genres = np.array(data_train.drop(['Id', 'Genre', 'Overview'],axis=1).columns)\n",
        "\n",
        "# List of overviews\n",
        "overviews_train = np.array(data_train['Overview'])\n",
        "overviews_test = np.array(data_test['Overview'])\n",
        "\n",
        "# Build the file locations for the images\n",
        "img_loc = \"/tmp/Multimodal_IMDB_dataset/Images\"\n",
        "\n",
        "img_locs_train = np.array([[img_loc + '/' + id + '.jpg' for id in data_train['Id']]])\n",
        "img_locs_test = [[img_loc + '/' + id + '.jpg' for id in data_test['Id']]]\n",
        "\n",
        "# This function is provided to read in the image files from the folder\n",
        "def parse_image(filename, label):\n",
        "    image = tf.io.read_file(filename)\n",
        "    image = tf.image.decode_jpeg(image)\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "m6sfJxXsxFm0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc6nKjDKez0T"
      },
      "source": [
        "\n",
        "### 1.a. Image processing of the posters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jdnTHGvDYSg8"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init4\n",
        "# Do not edit, remove, or copy this cell\n",
        "\n",
        "# Create the initial datasets of film posters\n",
        "list_posters_train_ds = tf.data.Dataset.from_tensor_slices((img_locs_train[0], labels_train))\n",
        "list_posters_valid_ds = tf.data.Dataset.from_tensor_slices((img_locs_test[0], labels_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3hIMuVQM4AN"
      },
      "source": [
        "* Create a function called ```img_process``` that converts the images to float32 datatype and resizes them to 64x64 pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BxtGcsAWM3gs"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1a-i\n",
        "### Create a function called img_process that converts the images to\n",
        "### float32 datatype and resizes them to 64x64 pixels\n",
        "\n",
        "def img_process(image, label):\n",
        "\n",
        "    #Complete here\n",
        "    image = tf.image.resize(image, [64, 64])  # Resize to 64x64\n",
        "    image = tf.cast(image, tf.float32)  # Convert to float32\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G51UPu0fNmNf"
      },
      "source": [
        "* **Using the ``tf.data`` API, load in the training and validation data for the posters. Be mindful of efficient data processing good practice to minimise the time it takes to load the data.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VNUS8p98Ph5L"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1a-ii\n",
        "### Use the parse_image and img_process functions to construct the training and\n",
        "### validation datasets. You should utilise good practice in optimising the\n",
        "### dataset loading. Use a batch size of 64.\n",
        "\n",
        "# Define the dataset pipeline for training and validation datasets\n",
        "posters_train_ds = (\n",
        "    list_posters_train_ds\n",
        "    # Parse the image and label using parse_image function\n",
        "    .map(lambda filename, label: tf.py_function(parse_image, [filename, label], [tf.uint8, tf.float32]),\n",
        "         num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Ensure the shape of the images is defined\n",
        "    .map(lambda image, label: (tf.ensure_shape(image, [None, None, None]), label),\n",
        "         num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Apply image preprocessing using img_process function\n",
        "    .map(img_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Batch the dataset\n",
        "    .batch(64)\n",
        "    # Optimize dataset loading using prefetching\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "posters_valid_ds = (\n",
        "    list_posters_valid_ds\n",
        "    # Parse the image and label using parse_image function\n",
        "    .map(lambda filename, label: tf.py_function(parse_image, [filename, label], [tf.uint8, tf.float32]),\n",
        "         num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Ensure the shape of the images is defined\n",
        "    .map(lambda image, label: (tf.ensure_shape(image, [None, None, None]), label),\n",
        "         num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Apply image preprocessing using img_process function\n",
        "    .map(img_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Batch the dataset\n",
        "    .batch(64)\n",
        "    # Optimize dataset loading using prefetching\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxLuHXhqqDrV"
      },
      "source": [
        "### 1.b. Natural Language processing of the overviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3JcCA19iLs34"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Init5\n",
        "# Do not edit, remove, or copy this cell\n",
        "\n",
        "# Create the initial datasets of the film overviews\n",
        "list_overviews_train_ds = tf.data.Dataset.from_tensor_slices((overviews_train, labels_train))\n",
        "list_overviews_valid_ds = tf.data.Dataset.from_tensor_slices((overviews_test, labels_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zc6y4JeOYw1Y"
      },
      "source": [
        "* **Using the ``tf.data`` API, load in the training and validation data for the overviews.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DejqXjATQLKY"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1b-i\n",
        "### Construct the training and validation datasets. Use a batch size of 64.\n",
        "\n",
        "overviews_train_ds = (\n",
        "    list_overviews_train_ds\n",
        "    .batch(64)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "overviews_valid_ds = (\n",
        "    list_overviews_valid_ds\n",
        "    .batch(64)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66neZRjDZzk-"
      },
      "source": [
        "* Build the vocabulary of the model by calling the ``encoder.adapt()`` method on the film overviews train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7rJr-DmWv-9q"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex1b-ii\n",
        "### Build the vocabulary of the model by calling the encoder.adapt() method on\n",
        "### the film overviews train data.\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
        "#Complete here\n",
        "encoder.adapt(overviews_train_ds.map(lambda text, label: text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nzcrSbhaLJY"
      },
      "source": [
        "* Print the first 200 words of the vocabulary you obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FunjHEkNv_mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d3505a-3349-4060-c756-2b372de1359a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'a', 'the', 'to', 'of', 'and', 'in', 'his', 'is', 'an', 'with', 'her', 'for', 'on', 'he', 'their', 'who', 'by', 'when', 'from', 'as', 'that', 'after', 'young', 'life', 'they', 'man', 'two', 'him', 'new', 'at', 'but', 'are', 'into', 'has', 'up', 'she', 'love', 'one', 'woman', 'out', 'family', 'find', 'must', 'friends', 'be', 'it', 'school', 'finds', 'story', 'them', 'world', 'where', 'about', 'group', 'while', 'have', 'girl', 'wife', 'lives', 'three', 'father', 'years', 'town', 'get', 'becomes', 'help', 'war', 'city', 'home', 'during', 'back', 'son', 'high', 'all', 'himself', 'only', 'gets', 'which', 'murder', 'york', 'boy', 'against', 'between', 'other', 'team', 'own', 'falls', 'american', 'former', 'mother', 'takes', 'daughter', 'police', 'will', 'tries', 'become', 'time', 'its', 'down', 'can', 'small', 'being', 'friend', 'take', 'order', 'this', 'together', 'college', 'set', 'before', 'goes', 'agent', 'save', 'old', 'over', 'through', 'was', 'death', 'relationship', 'not', 'meets', 'each', 'more', 'way', 'husband', 'couple', 'go', 'first', 'people', 'comes', 'killer', 'off', 'day', 'mysterious', 'hes', 'brother', 'best', 'job', 'returns', 'make', 'drug', 'discovers', 'cop', 'what', 'us', 'men', 'been', 'house', 'now', 'living', 'forced', 'parents', 'four', 'begins', 'past', 'secret', 'just', 'student', 'film', 'evil', 'detective', 'work', 'trying', 'stop', 'turns', 'local', 'discover', 'so', 'sent', 'whose', 'teenage', 'married', 'or', 'live', 'women', 'fight', 'movie', 'most', 'than', 'prison', 'brothers', 'some', 'there', 'earth', 'try', 'true', 'then', 'series', 'children', 'black', 'wants', 'come', 'soon', 'trip', 'star', 'herself', 'both', 'battle']\n"
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Ex1b-iii\n",
        "### Print the first 200 words of the vocabulary you obtained.\n",
        "\n",
        "#Complete here\n",
        "print(encoder.get_vocabulary()[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Uo36n_KGYb"
      },
      "source": [
        "# 2. Definition of the models\n",
        "\n",
        "### 2.a. CNN\n",
        "\n",
        "**Using the Keras Functional API, create a convolutional neural network with the architecture shown in the model summary below.**\n",
        "\n",
        "**A few important points to consider:**\n",
        "\n",
        "* Call the convolutional layers and the first dense layer should have ReLU activation functions. The output layer should have a Sigmoid activation function.\n",
        "* Pay attention to the output shapes and the number of partmeters for each layer, as these give indications as to the correct settings for the number of filters, kernel size, stride length and padding.\n",
        "* Use the layer names provided in the summary in your model.\n",
        "* For the dropout layers, use a dropout rate of 0.2 after the convolutional layers and 0.5 after the dense layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBSBfH-QP6M0"
      },
      "source": [
        "\n",
        "```\n",
        "# Model Summary\n",
        "\n",
        "Model: \"model\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " Input (InputLayer)          [(None, 64, 64, 3)]       0         \n",
        "                                                                 \n",
        " Conv0 (Conv2D)              (None, 32, 32, 16)        448       \n",
        "                                                                 \n",
        " Drop1 (Dropout)             (None, 32, 32, 16)        0         \n",
        "                                                                 \n",
        " Conv1 (Conv2D)              (None, 32, 32, 32)        4640      \n",
        "                                                                 \n",
        " Conv2 (Conv2D)              (None, 32, 32, 32)        9248      \n",
        "                                                                 \n",
        " Drop2 (Dropout)             (None, 32, 32, 32)        0         \n",
        "                                                                 \n",
        " Pool1 (MaxPooling2D)        (None, 16, 16, 32)        0         \n",
        "                                                                 \n",
        " Conv3 (Conv2D)              (None, 16, 16, 64)        18496     \n",
        "                                                                 \n",
        " Conv4 (Conv2D)              (None, 16, 16, 64)        36928     \n",
        "                                                                 \n",
        " Drop3 (Dropout)             (None, 16, 16, 64)        0         \n",
        "                                                                 \n",
        " Pool2 (MaxPooling2D)        (None, 8, 8, 64)          0         \n",
        "                                                                 \n",
        " Conv5 (Conv2D)              (None, 8, 8, 128)         73856     \n",
        "                                                                 \n",
        " Conv6 (Conv2D)              (None, 8, 8, 128)         147584    \n",
        "                                                                 \n",
        " Drop4 (Dropout)             (None, 8, 8, 128)         0         \n",
        "                                                                 \n",
        " Pool3 (MaxPooling2D)        (None, 4, 4, 128)         0         \n",
        "                                                                 \n",
        " Flat (Flatten)              (None, 2048)              0         \n",
        "                                                                 \n",
        " FC1 (Dense)                 (None, 1024)              2098176   \n",
        "                                                                 \n",
        " Drop5 (Dropout)             (None, 1024)              0         \n",
        "                                                                 \n",
        " FC2 (Dense)                 (None, 1024)              1049600   \n",
        "                                                                 \n",
        " Drop6 (Dropout)             (None, 1024)              0         \n",
        "                                                                 \n",
        " Output (Dense)              (None, 25)                25625     \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 3464601 (13.22 MB)\n",
        "Trainable params: 3464601 (13.22 MB)\n",
        "Non-trainable params: 0 (0.00 Byte)\n",
        "_________________________________________________________________\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "im9qipCiKJMK"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2a-i\n",
        "### Define the model using the Keras Functional API. Use the summary above as a\n",
        "### guide for the model parameters. You will need to define the filters/units of\n",
        "### the layers correctly, as well as the kernel size, stride length and padding\n",
        "### of the convolutional layers.\n",
        "\n",
        "def build_cnn_model():\n",
        "    input_tensor = Input(shape=(64, 64, 3), name='Input')\n",
        "\n",
        "    x = Conv2D(16, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu', name='Conv0')(input_tensor)\n",
        "    x = Dropout(0.2, name='Drop1')(x)\n",
        "    x = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', name='Conv1')(x)\n",
        "    x = Conv2D(32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', name='Conv2')(x)\n",
        "    x = Dropout(0.2, name='Drop2')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='Pool1')(x)\n",
        "    x = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', name='Conv3')(x)\n",
        "    x = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', name='Conv4')(x) # Complete Conv4 layer definition\n",
        "    x = Dropout(0.2, name='Drop3')(x)                                                                      # Add Drop3 layer\n",
        "    x = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid', name='Pool2')(x)                   # Add Pool2 layer\n",
        "    x = Flatten(name='Flatten')(x)                                                                         # Add Flatten layer\n",
        "    x = Dense(25, activation='softmax', name='Output')(x)                                                  # Add Output layer with 25 units and softmax activation\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_tensor, outputs=x, name='CNN_Model')\n",
        "\n",
        "    return model\n",
        "\n",
        "cnn_model = build_cnn_model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArMjXi7iUxn9"
      },
      "source": [
        "* Print the model summary and confirm it has the same architecture as the one provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4ogeFQCqnz06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "342efaee-50db-4ee6-cf4c-814e47706d98"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"CNN_Model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_Model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (\u001b[38;5;33mInputLayer\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv0 (\u001b[38;5;33mConv2D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m448\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop1 (\u001b[38;5;33mDropout\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv1 (\u001b[38;5;33mConv2D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m4,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv2 (\u001b[38;5;33mConv2D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │           \u001b[38;5;34m9,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop2 (\u001b[38;5;33mDropout\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Pool1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv3 (\u001b[38;5;33mConv2D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv4 (\u001b[38;5;33mConv2D\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop3 (\u001b[38;5;33mDropout\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Pool2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4096\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Output (\u001b[38;5;33mDense\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)                  │         \u001b[38;5;34m102,425\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ Input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Pool1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Conv4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Drop3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Pool2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4096</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)                  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">102,425</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m172,185\u001b[0m (672.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,185</span> (672.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m172,185\u001b[0m (672.60 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">172,185</span> (672.60 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# CodeGrade Tag Ex2a-ii\n",
        "### Print the model summary and confirm it has the same architecture as the one\n",
        "### provided.\n",
        "\n",
        "#Complete here\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q4qQnsuP-CM"
      },
      "source": [
        "* **Compile the model using the Adam Optimizer with a learning rate of ```1e-4``` and ```binary crossentropy``` loss function. For the metrics, use the ``Precision`` and ``Recall`` functions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qndXNKsyQl2G"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2a-iii\n",
        "### Compile the model using the Adam Optimizer with a learning rate of 1e-4 and\n",
        "### binary crossentropy loss function. For the metrics, use the Precision and\n",
        "### Recall functions.\n",
        "\n",
        "cnn_model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[Precision(), Recall()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMLNubJ4q2iS"
      },
      "source": [
        "### 2.b. LSTM model\n",
        "\n",
        "* Set up the embedding layer by using ```tf.keras.layers.Embedding```. The ```input_dim``` is the length of the vocab, and the ```output_dim``` must be **265**. You should also set ```mask_zero=True```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v_kXVZozwQbR"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2b-i\n",
        "### Set up the embedding layer. The input_dim is the length of the vocab, and\n",
        "### the output_dim must be 256. You should also set mask_zero=True.\n",
        "\n",
        "embedder = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=256, mask_zero=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtTvirSLateH"
      },
      "source": [
        "* Use ```tf.keras.Sequential``` to build a keras sequential model, with the following layers:\n",
        "\n",
        "\n",
        "\n",
        "  1.   encoder\n",
        "  2.   embedder\n",
        "  3.   biLSTM layer with 256 units, dropout 0.5, recurrent dropout 0.2 (make sure to use the right ```return_sequences``` parametre to be able to stack this layer with the following BiLSTM)\n",
        "  4.   biLSTM layer with 128 units, dropout 0.5, recurrent dropout 0.2\n",
        "  5.   dense layer with 128 units and relu activation function\n",
        "  6.   dropout with rate 0.8\n",
        "  7.   dense output layer with 25 units and sigmoid activation function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dED-ES8KwRMp"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2b-ii\n",
        "### Build a keras sequential model, with the layers provided above.\n",
        "\n",
        "lstm_model = keras.Sequential([\n",
        "    encoder,\n",
        "    embedder,\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(256, dropout=0.5, recurrent_dropout=0.2, return_sequences=True)),\n",
        "    keras.layers.Bidirectional(keras.layers.LSTM(128, dropout=0.5, recurrent_dropout=0.2)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dropout(0.8),\n",
        "    keras.layers.Dense(25, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.build((None,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A_Dugs1dxDy"
      },
      "source": [
        "* Print the model summary and confirm is has the same architecture as the outline provided above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RlLWD9ZUDQAD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "2a378d49-f301-4d1e-96d3-dbd67b68dbd6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTextVectorization\u001b[0m)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │       \u001b[38;5;34m2,560,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)           │       \u001b[38;5;34m1,050,624\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m656,384\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)                  │           \u001b[38;5;34m3,225\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ text_vectorization                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)                  │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">656,384</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,303,129\u001b[0m (16.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,303,129</span> (16.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,303,129\u001b[0m (16.42 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,303,129</span> (16.42 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# CodeGrade Tag Ex2b-iii\n",
        "### Print the model summary and confirm is has the same architecture as the\n",
        "### outline provided above.\n",
        "\n",
        "#Complete here\n",
        "lstm_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1ZyLIvvbF_t"
      },
      "source": [
        "* Compile the model with binary crossentropy loss and the adam optimizer. For the metrics, use the Precision and Recall functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UfgwlJYFwVO6"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex2b-iv\n",
        "### Compile the model with binary crossentropy loss, the adam optimizer, with\n",
        "### the precision and recall metrics\n",
        "\n",
        "lstm_model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[Precision(), Recall()]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G9gILWpKJXP"
      },
      "source": [
        "# 3. Training of the models\n",
        "\n",
        "* **For each model, create a Checkpoint Callback that saves the weights of the best performing epoch, based on the validation loss.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ulXjHaiZKLLZ"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex3a-i\n",
        "### Create two ModelCheckpoint callbacks to store the bext weights from each\n",
        "### model, both based on the validation loss.\n",
        "\n",
        "checkpoint_cnn_filepath = '/content/checkpoint_cnn.weights.h5'\n",
        "checkpoint_lstm_filepath = '/content/checkpoint_lstm.weights.h5'\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_cnn_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_cnn_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n",
        "\n",
        "checkpoint_lstm_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_lstm_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iUsL966RQMJ"
      },
      "source": [
        "* **Create a Learning Rate Scheduler Callback that utilises the provided function to decrease the learning rate during training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5jhC3C7qRX1B"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex3a-ii\n",
        "### Using the function provided, create a LearningRateScheduler callback, call\n",
        "### it \"lr_callback\"\n",
        "\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return float(lr)\n",
        "    else:\n",
        "        return float(lr * tf.math.exp(-0.01))\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQbqwZNjRYIC"
      },
      "source": [
        "### 3.a. CNN training\n",
        "\n",
        "* **Train the CNN model for 40 epochs, using the callbacks you made previously. Store the losses and metrics to use later.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "05qEIpORRfr8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "8518ce6a-3617-4028-9f56-ccd4790bc164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot take the length of shape with unknown rank.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fd8f249eef38>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m cnn_history = cnn_model.fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mposters_train_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposters_valid_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/losses/loss.py\u001b[0m in \u001b[0;36msqueeze_or_expand_to_same_rank\u001b[0;34m(x1, x2, expand_rank_1)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqueeze_or_expand_to_same_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_rank_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;34m\"\"\"Squeeze/expand last dim if ranks differ from expected by exactly 1.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mx1_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mx2_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx1_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx2_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot take the length of shape with unknown rank."
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Ex3a-iii\n",
        "### Train the model for 40 epochs, using the callbacks you have created. Store\n",
        "### the losses and metrics in a history object.\n",
        "\n",
        "# Train the model\n",
        "cnn_history = cnn_model.fit(\n",
        "    posters_train_ds,\n",
        "    validation_data=posters_valid_ds,\n",
        "    epochs=40,\n",
        "    callbacks=[checkpoint_cnn_callback, lr_callback]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6SP4T-qxWUj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEMx5HScjqG5"
      },
      "source": [
        "* **Train the model for 20 epochs** (this may take several minutes)**, using the callbacks you made previously. Store the losses and metrics to use later.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS21uKCBjqdt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157b8f21-25ff-41dd-a163-122b495ca678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 498ms/step - loss: 0.4925 - precision_1: 0.1833 - recall_1: 0.3582 - val_loss: 0.2462 - val_precision_1: 0.5309 - val_recall_1: 0.2217 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 471ms/step - loss: 0.2866 - precision_1: 0.4101 - recall_1: 0.2125 - val_loss: 0.2391 - val_precision_1: 0.5380 - val_recall_1: 0.2200 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 465ms/step - loss: 0.2638 - precision_1: 0.4857 - recall_1: 0.2134 - val_loss: 0.2280 - val_precision_1: 0.6249 - val_recall_1: 0.1976 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 477ms/step - loss: 0.2446 - precision_1: 0.5558 - recall_1: 0.2572 - val_loss: 0.2222 - val_precision_1: 0.6278 - val_recall_1: 0.2279 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 474ms/step - loss: 0.2284 - precision_1: 0.5945 - recall_1: 0.3024 - val_loss: 0.2160 - val_precision_1: 0.6486 - val_recall_1: 0.2445 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 481ms/step - loss: 0.2156 - precision_1: 0.6339 - recall_1: 0.3264 - val_loss: 0.2138 - val_precision_1: 0.6317 - val_recall_1: 0.2725 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 498ms/step - loss: 0.2021 - precision_1: 0.6559 - recall_1: 0.3680 - val_loss: 0.2127 - val_precision_1: 0.6335 - val_recall_1: 0.2924 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m62/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m7s\u001b[0m 444ms/step - loss: 0.1886 - precision_1: 0.6914 - recall_1: 0.4082"
          ]
        }
      ],
      "source": [
        "# CodeGrade Tag Ex3b-i\n",
        "# Train the model for 20 epochs.\n",
        "lstm_history = lstm_model.fit(\n",
        "    overviews_train_ds,\n",
        "    validation_data=overviews_valid_ds,\n",
        "    epochs=20,\n",
        "    callbacks=[checkpoint_lstm_callback, lr_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkaP8sRwKLYn"
      },
      "source": [
        "# 4. Evaluation of the models\n",
        "\n",
        "### 4.a. CNN Evaluation\n",
        "\n",
        "* **Create plots using the losses and metrics. In your report, discuss these results and critically evaluate the model performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-zTj4ZZKQ2p"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex4a-i\n",
        "\n",
        "#Complete here\n",
        "# Plot training & validation loss values\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.plot(cnn_history.history['loss'])\n",
        "# plt.plot(cnn_history.history['val_loss'])\n",
        "# plt.title('Model loss')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "# plt.show()\n",
        "\n",
        "# Plot training & validation precision values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cnn_history.history['precision'])\n",
        "plt.plot(cnn_history.history['val_precision'])\n",
        "plt.title('Model precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation recall values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(cnn_history.history['recall'])\n",
        "plt.plot(cnn_history.history['val_recall'])\n",
        "plt.title('Model recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be9li89iSP6E"
      },
      "source": [
        "* **Load the best weights from your model checkpoint, and create plots demonstrating the classification performance for all three classes. Include these plots in your report, and critically evaluate on the performance of the model across the classes.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMqFb3cGaO2B"
      },
      "source": [
        "### 4.b. LSTM Evaluation\n",
        "\n",
        "* **Create plots using the losses and metrics. In your report, discuss these results and critically evaluate the model performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRa83gpJU35q"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex4b-i\n",
        "\n",
        "#Complete here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # CodeGrade Tag Ex4b-i\n",
        "# #Complete here\n",
        "\n",
        "#Complete here\n",
        "# Assuming lstm_history is available from previous training\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "# Check if 'precision' exists in history before plotting\n",
        "if 'precision' in lstm_history.history:\n",
        "    plt.plot(lstm_history.history['precision'])\n",
        "    plt.plot(lstm_history.history['val_precision'])\n",
        "    plt.title('LSTM Model precision')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "else:\n",
        "    print(\"Precision metric not found in history. Ensure it was used during model compilation.\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# Check if 'recall' exists in history before plotting\n",
        "if 'recall' in lstm_history.history:\n",
        "    plt.plot(lstm_history.history['recall'])\n",
        "    plt.plot(lstm_history.history['val_recall'])\n",
        "    plt.title('LSTM Model recall')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "else:\n",
        "    print(\"Recall metric not found in history. Ensure it was used during model compilation.\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "T2F6WvcE3vLq",
        "outputId": "175be1f5-da68-40ba-dc7a-a075131634bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-09d8e68667fe>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Assuming lstm_history is available from previous training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Check if 'precision' exists in history before plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxGqxM-kTBck"
      },
      "source": [
        "### 4.c. Produce examples for the report\n",
        "\n",
        "* First, load the best weights from your checkpoints of both your models.\n",
        "\n",
        "* Choose a few films from the dataset, plot their posters and print their overviews. Use these example films to demonstrate the classification performance of the CNN model on their posters and of the LSTM model on their overview.\n",
        "\n",
        "* Be sure to demonstrate the results of the multi-label classification. Compare, for each example film, the top three most probable genres predicted by the CNN and the top three most probable genres predicted by the LSTM with the ground truth genres.\n",
        "\n",
        "* Include these examples in your report, and critically evaluate on the performance of the model across the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7BtdNrsSlai"
      },
      "outputs": [],
      "source": [
        "# CodeGrade Tag Ex4c\n",
        "\n",
        "#Complete here"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pLD7BilAoFBC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}